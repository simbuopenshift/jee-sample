import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryException;
import com.google.cloud.bigquery.BigQueryOptions;
import com.google.cloud.bigquery.CsvOptions;
import com.google.cloud.bigquery.Job;
import com.google.cloud.bigquery.JobInfo;
import com.google.cloud.bigquery.TableId;
import org.springframework.stereotype.Service;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.util.List;
import java.util.Map;
import java.util.UUID;

@Service
public class BigQueryService {

    private static final String PROJECT_ID = "your-project-id";
    private static final String DATASET_ID = "your-dataset-id";
    private static final String TABLE_ID = "your-table-id";
    private static final int BATCH_SIZE = 1000; // Adjust batch size as needed

    public void insertData(List<Map<String, Object>> data) {
        BigQuery bigquery = BigQueryOptions.newBuilder().setProjectId(PROJECT_ID).build().getService();

        // Split data into batches
        for (int i = 0; i < data.size(); i += BATCH_SIZE) {
            int endIndex = Math.min(i + BATCH_SIZE, data.size());
            List<Map<String, Object>> batch = data.subList(i, endIndex);
            insertBatch(bigquery, batch);
        }
    }

    private void insertBatch(BigQuery bigquery, List<Map<String, Object>> batch) {
        String csvData = convertToCSV(batch);
        try {
            String jobId = UUID.randomUUID().toString();
            TableId tableId = TableId.of(PROJECT_ID, DATASET_ID, TABLE_ID);

            Job job = bigquery.create(JobInfo.newBuilder(JobInfo.of(tableId))
                    .setLoadJobConfig(JobInfo.LoadJobConfiguration.newBuilder(new ByteArrayInputStream(csvData.getBytes(StandardCharsets.UTF_8)))
                            .setFormatOptions(CsvOptions.newBuilder().setFieldDelimiter(",").build())
                            .build())
                    .setJobId(jobId)
                    .build());

            // Wait for the job to complete
            job.waitFor();
            if (job.getStatus().getError() != null) {
                throw new BigQueryException(500, "Error loading data into BigQuery: " + job.getStatus().getError());
            }

            System.out.println("Batch inserted successfully.");
        } catch (InterruptedException | IOException | BigQueryException e) {
            System.out.println("Error inserting data: " + e.getMessage());
        }
    }

    private String convertToCSV(List<Map<String, Object>> data) {
        StringBuilder csv = new StringBuilder();
        // Assuming all rows have the same columns in the same order
        if (!data.isEmpty()) {
            Map<String, Object> firstRow = data.get(0);
            firstRow.keySet().forEach(column -> csv.append(column).append(","));
            csv.deleteCharAt(csv.length() - 1); // Remove trailing comma
            csv.append("\n");
        }

        for (Map<String, Object> row : data) {
            row.values().forEach(value -> csv.append(value).append(","));
            csv.deleteCharAt(csv.length() - 1); // Remove trailing comma
            csv.append("\n");
        }
        return csv.toString();
    }
}
