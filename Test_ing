import com.google.cloud.bigquery.*;
import org.springframework.stereotype.Service;

import java.io.ByteArrayInputStream;
import java.nio.charset.StandardCharsets;
import java.util.List;
import java.util.Map;
import java.util.UUID;

@Service
public class BigQueryService {

    private static final String PROJECT_ID = "your-project-id";
    private static final String DATASET_ID = "your-dataset-id";
    private static final String TABLE_ID = "your-table-id";
    private static final int BATCH_SIZE = 1000; // Adjust batch size as needed

    public void insertData(List<Map<String, Object>> data) {
        BigQuery bigQuery = BigQueryOptions.getDefaultInstance().getService();

        // Split data into batches
        for (int i = 0; i < data.size(); i += BATCH_SIZE) {
            int endIndex = Math.min(i + BATCH_SIZE, data.size());
            List<Map<String, Object>> batch = data.subList(i, endIndex);
            insertBatch(bigQuery, batch);
        }
    }

    private void insertBatch(BigQuery bigQuery, List<Map<String, Object>> batch) {
        String csvData = convertToCSV(batch);
        try {
            String jobId = UUID.randomUUID().toString();
            TableId tableId = TableId.of(PROJECT_ID, DATASET_ID, TABLE_ID);

            Job job = bigQuery.create(JobInfo.of(JobId.of(jobId),
                    LoadJobConfiguration.newBuilder(tableId)
                            .setFormatOptions(FormatOptions.csv())
                            .build()));

            // Provide data to the job
            LoadJobConfiguration configuration = job.getConfiguration();
            LoadJobConfiguration loadJobConfiguration = configuration.toBuilder()
                    .setFormatOptions(FormatOptions.csv())
                    .build();
            ByteArrayInputStream contentStream = new ByteArrayInputStream(csvData.getBytes(StandardCharsets.UTF_8));
            Job loadJob = job.toBuilder().setConfiguration(loadJobConfiguration).build().load(contentStream);

            // Wait for the job to complete
            loadJob = loadJob.waitFor();
            if (loadJob.getStatus().getError() != null) {
                throw new BigQueryException(500, "Error loading data into BigQuery: " + loadJob.getStatus().getError());
            }

            System.out.println("Batch inserted successfully.");
        } catch (InterruptedException | BigQueryException e) {
            System.out.println("Error inserting data: " + e.getMessage());
        }
    }

    private String convertToCSV(List<Map<String, Object>> data) {
        StringBuilder csv = new StringBuilder();
        // Assuming all rows have the same columns in the same order
        if (!data.isEmpty()) {
            Map<String, Object> firstRow = data.get(0);
            firstRow.keySet().forEach(column -> csv.append(column).append(","));
            csv.deleteCharAt(csv.length() - 1); // Remove trailing comma
            csv.append("\n");
        }

        for (Map<String, Object> row : data) {
            row.values().forEach(value -> csv.append(value).append(","));
            csv.deleteCharAt(csv.length() - 1); // Remove trailing comma
            csv.append("\n");
        }
        return csv.toString();
    }
}
